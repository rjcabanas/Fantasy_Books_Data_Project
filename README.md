**What is the Project?**

This project collects all available data on fantasy books stored within the Open Library API. This includes details about the books, authors and reviews/ratings provided by Open Library API users.

**Features and Technology**

  - Data ingestion Pipeline (Apache Airflow, Python Packages: requests, pandas, json, os, azure, dotenv):
    - Connecting to the API
    - Requesting for the information on Fantasy books (Books, Authors, Book Reviews/Ratings)
    - Storing paginated JSON files as a combined PARQUET file
    - Pushing the PARQUET files into an Azure Data Lake Gen 2 Container (Bronze layer)
    - Orchestration carried out using a Python written DAG
  
  - Data Transformation (Databricks Notebooks, PySpark):
    -  Connecting Databricks to Azure Data Lake Gen 2 with an Entra ID app with "Storage Blob Contributor" role for the storage account
    -  Reading the Bronze layer, cleaning the data and pushing it into the Silver layer
   
  - Dimensional Modelling (Databricks Notebooks, PySpark):
    - Creating a widget to create a parameter that indicates whether the notebook wll be run as an initial run or an incremental run
    - Relevant columns from the Silver layer and a new "DimKey" column are selected to create the schema for the dimension tables
    - The notebook checks whether existing dimension tables already exist to decide how data will be loaded (it will be empty if it's initial and existing data is shown otherwise)
    - Existing and new records are separated into two table (new records will have NULL values for the "DimKey" column)
    - Surrogate/Dimension Keys are generated by first checking the widget to verify if the notebook is running as an initial run or an incremental run. The "DimKey" will start from 1 if it's an initial run or the max of "DimKey" + 1 if it's an incremental run.
    - The existing and new records tables are then union joined to create the final dimension table
    - Slowly Changing Dimensions Type 1 was implemented to account for potential future changes to existing records

  - Creating the Fact Table (Databricks Notebooks, PySpark):
    -  
