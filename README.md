**Project Overview**

This project collects data related to fantasy books from the Open Library API, including metadata on books, authors, and user reviews/ratings. The data is processed and stored using a medallion architecture (Bronze, Silver, Gold layers) on Azure Data Lake Gen2, with orchestration and analytics pipelines implemented using industry-standard tools.

**Tech Stack**
- Orchestration: Apache Airflow, Databricks Workflows
- Data Processing: Python, PySpark, Pandas
- Storage: Azure Data Lake Gen2
- Authentication: Azure Entra ID with "Storage Blob Contributor" role
- Visualization: Power BI
- File Formats: JSON (raw), Parquet (processed)

**Architecture: Medallion Layers**
- Bronze Layer: Raw ingested data from API, stored as combined Parquet files
- Silver Layer: Cleaned, structured data ready for modeling
- Gold Layer: Dimensional and fact tables optimized for analytics

**Pipeline Stages**

1. Data Ingestion (Apache Airflow + Python)
- Connects to the Open Library API
- Python scripts includes error handling with API rate limits
- Fetches paginated data for:
  - Fantasy books
  - Authors
  - Book reviews and ratings
- Stores the JSON responses locally
- Converts combined JSON into a Parquet file
- Uploads the Parquet file to the Bronze layer in Azure Data Lake Gen2
- Ingestion orchestration is handled using Apache Airflow

![image](https://github.com/user-attachments/assets/e74e6a4e-3d5c-4624-b9b3-702a5188ae41)

Apache Airflow DAG run screenshot.

2. ðŸ§¹ Data Transformation (Databricks + PySpark)
a. Data Cleaning
  - Databricks connects to Azure Data Lake Gen2 using an Entra ID app
  - Reads Parquet files from the Bronze layer
  - Cleans and normalizes the dataset
  - Writes processed data to the Silver layer

b. Dimensional Modeling
  - Uses widgets to control run type: initial or incremental
  - Creates dimension tables with selected fields and a new DimKey (surrogate key)
  - Checks if a dimension table already exists:
    - If initial run: DimKey starts from 1
    - If incremental: continues from the max DimKey
  - Separates new and existing records
  - Applies Slowly Changing Dimension Type 1 to handle updates
  - Combines records to generate updated dimension tables

c. Fact Table Creation
  - Reads ratings data from the Silver layer
  - Joins dimension keys from dimension tables
  - Drops natural keys from the fact table
  - Writes to the Gold layer:
    - If no existing fact table: overwrite mode
    - If it exists: upsert logic is used
  - Controlled via Databricks Workflows, which pass parameters to notebooks

![image](https://github.com/user-attachments/assets/fd7838b6-043a-4f74-899d-d058286f0014)
Databricks Workflows run screenshot.

3. ðŸ“Š Data Analytics (Power BI)
- Power BI connects to the Gold layer using the Parquet Connector
- Alternatively, Azure Synapse Analytics can be used to access Gold data
- Visualizations include:
  - Most popular books
  - Best-rated authors
- Sample dashboard available in the PowerBI_Files folder

**GitHub Repo Folder Structure**



************************************************



  - Data ingestion Pipeline (Apache Airflow, Python Packages: requests, pandas, json, os, azure, dotenv):
    - Connecting to the API
    - Requesting for the information on Fantasy books (Books, Authors, Book Reviews/Ratings)
    - Storing paginated JSON files as a combined PARQUET file
    - Pushing the PARQUET files into an Azure Data Lake Gen 2 Container (Bronze layer)
    - Orchestration carried out using Apache Airflow
   
    ![image](https://github.com/user-attachments/assets/e74e6a4e-3d5c-4624-b9b3-702a5188ae41)

    Apache Airflow DAG run screenshot.
  
  - Data Transformation Pipeline (Databricks Notebooks, PySpark, Databricks Workflows):
    - Data Cleaning:
      - Connecting Databricks to Azure Data Lake Gen 2 with an Entra ID app with "Storage Blob Contributor" role for the storage account
      - Reading the Bronze layer, cleaning the data and pushing it into the Silver layer
   
    - Dimensional Modelling:
      - Creating a widget to create a parameter that indicates whether the notebook wll be run as an initial run or an incremental run
      - Relevant columns from the Silver layer and a new "DimKey" column are selected to create the schema for the dimension tables
      - The notebook checks whether existing dimension tables already exist to decide how data will be loaded (it will be empty if it's initial and existing data is shown              otherwise)
      - Existing and new records are separated into two table (new records will have NULL values for the "DimKey" column)
      - Surrogate/Dimension Keys are generated by first checking the widget to verify if the notebook is running as an initial run or an incremental run. The "DimKey" will             start from 1 if it's an initial run or the max of "DimKey" + 1 if it's an incremental run.
      - The existing and new records tables are then union joined to create the final dimension table
      - Slowly Changing Dimensions Type 1 was implemented to account for potential future changes to existing records
  
    - Creating the Fact Table:
      -  Silver layer ratings file is read along with the created dimension tables
      -  Dimension keys are brought in from dimension tables by creating a left join on the primary keys avaialable in both the fact table and corresponding dimension table
      -  To finalise the fact table, primary keys are dropped from the table
      -  Before saving the fact table, the gold container is checked to see if there is an existing fact table. If there is no existing table, data is saved using "overwrite".          However, if there is an existing table, then the new data is upserted to the existing table
      -  Orchestration carried out using Databricks Workflows (parameters are created here to control the widget within the relevant notebooks for incremental runs)

      ![image](https://github.com/user-attachments/assets/fd7838b6-043a-4f74-899d-d058286f0014)
      Databricks Workflows run screenshot.

  -  Data Analytics (PowerBI):
    - PowerBI is connected to the tables by using a Parquet Connector directly to the Gold Layer files to access the data (Alternatively, Azure Synapse Analytics can be used         to directly connect with the storage account to access the data for analytical work).
    - A simple dashboard was created to showcase the most popular and best rated books and authors (refer to the                   "PowerBI_Files" folder).
