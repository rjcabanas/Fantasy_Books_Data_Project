**What is the Project?**

This project collects all available data on fantasy books stored within the Open Library API. This includes details about the books, authors and reviews/ratings provided by Open Library API users.

**Features and Technology**

  - Data ingestion Pipeline (Apache Airflow, Python Packages: requests, pandas, json, os, azure, dotenv):
    - Connecting to the API
    - Requesting for the information on Fantasy books (Books, Authors, Book Reviews/Ratings)
    - Storing paginated JSON files as a combined PARQUET file
    - Pushing the PARQUET files into an Azure Data Lake Gen 2 Container (Bronze layer)
    - Orchestration carried out using Apache Airflow
   
    ![image](https://github.com/user-attachments/assets/e74e6a4e-3d5c-4624-b9b3-702a5188ae41)
    Apache Airflow DAG run screenshot.
  
  - Data Transformation Pipeline (Databricks Notebooks, PySpark, Databricks Workflows):
    - Data Cleaning:
      - Connecting Databricks to Azure Data Lake Gen 2 with an Entra ID app with "Storage Blob Contributor" role for the storage account
      - Reading the Bronze layer, cleaning the data and pushing it into the Silver layer
   
    - Dimensional Modelling:
      - Creating a widget to create a parameter that indicates whether the notebook wll be run as an initial run or an incremental run
      - Relevant columns from the Silver layer and a new "DimKey" column are selected to create the schema for the dimension tables
      - The notebook checks whether existing dimension tables already exist to decide how data will be loaded (it will be empty if it's initial and existing data is shown              otherwise)
      - Existing and new records are separated into two table (new records will have NULL values for the "DimKey" column)
      - Surrogate/Dimension Keys are generated by first checking the widget to verify if the notebook is running as an initial run or an incremental run. The "DimKey" will             start from 1 if it's an initial run or the max of "DimKey" + 1 if it's an incremental run.
      - The existing and new records tables are then union joined to create the final dimension table
      - Slowly Changing Dimensions Type 1 was implemented to account for potential future changes to existing records
  
    - Creating the Fact Table:
      -  Silver layer ratings file is read along with the created dimension tables
      -  Dimension keys are brought in from dimension tables by creating a left join on the primary keys avaialable in both the fact table and corresponding dimension table
      -  To finalise the fact table, primary keys are dropped from the table
      -  Before saving the fact table, the gold container is checked to see if there is an existing fact table. If there is no existing table, data is saved using "overwrite".          However, if there is an existing table, then the new data is upserted to the existing table
      -  Orchestration carried out using Databricks Workflows (parameters are created here to control the widget within the relevant notebooks for incremental runs)

      ![image](https://github.com/user-attachments/assets/fd7838b6-043a-4f74-899d-d058286f0014)
      Databricks Workflows run screenshot.

  -  Data Analytics (PowerBI):
    - PowerBI is connected to the tables by using a Parquet Connector directly to the Gold Layer files to access the data (Alternatively, Azure Synapse Analytics can be used         to directly connect with the storage account to access the data for analytical work).
    - A simple dashboard was created to showcase the most popular and best rated books and authors (refer to the                   "PowerBI_Files" folder).
